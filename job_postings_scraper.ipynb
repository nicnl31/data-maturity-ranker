{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd67f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035f1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/asx_listed_cleaned.csv', index_col=0).sort_values(by='Market Cap', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796cc084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Company</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csl</td>\n",
       "      <td>csl ltd</td>\n",
       "      <td>health care</td>\n",
       "      <td>140501000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cba</td>\n",
       "      <td>commonwealth bank of australia</td>\n",
       "      <td>financials</td>\n",
       "      <td>110976000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bhp</td>\n",
       "      <td>bhp group ltd</td>\n",
       "      <td>materials</td>\n",
       "      <td>95298300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wbc</td>\n",
       "      <td>westpac banking corporation</td>\n",
       "      <td>financials</td>\n",
       "      <td>58798200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nab</td>\n",
       "      <td>national australia bank ltd</td>\n",
       "      <td>financials</td>\n",
       "      <td>50611200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Code                         Company       Sector    Market Cap\n",
       "0  csl                         csl ltd  health care  140501000000\n",
       "1  cba  commonwealth bank of australia   financials  110976000000\n",
       "2  bhp                   bhp group ltd    materials   95298300000\n",
       "3  wbc     westpac banking corporation   financials   58798200000\n",
       "4  nab     national australia bank ltd   financials   50611200000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7acc95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['csl', 'cba', 'bhp', 'wow', 'tls']\n",
    "company_mask = data['Code'].isin(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d54016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Company</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>csl</td>\n",
       "      <td>csl ltd</td>\n",
       "      <td>health care</td>\n",
       "      <td>140501000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cba</td>\n",
       "      <td>commonwealth bank of australia</td>\n",
       "      <td>financials</td>\n",
       "      <td>110976000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bhp</td>\n",
       "      <td>bhp group ltd</td>\n",
       "      <td>materials</td>\n",
       "      <td>95298300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wow</td>\n",
       "      <td>woolworths group ltd</td>\n",
       "      <td>consumer staples</td>\n",
       "      <td>45155400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tls</td>\n",
       "      <td>telstra corporation ltd</td>\n",
       "      <td>other</td>\n",
       "      <td>36274600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code                         Company            Sector    Market Cap\n",
       "0   csl                         csl ltd       health care  140501000000\n",
       "1   cba  commonwealth bank of australia        financials  110976000000\n",
       "2   bhp                   bhp group ltd         materials   95298300000\n",
       "6   wow            woolworths group ltd  consumer staples   45155400000\n",
       "11  tls         telstra corporation ltd             other   36274600000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_work = data[company_mask]\n",
    "data_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c91a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_rows(df):\n",
    "    return df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e78e88",
   "metadata": {},
   "source": [
    "# Build a Scraper custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbc4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseScraper(object):\n",
    "    \"\"\"\n",
    "    The base scraper class, which stores the data and the URL for child scraper classes.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, raw_url):\n",
    "        self.raw_url = raw_url\n",
    "        self.data = []\n",
    "        \n",
    "    def save_data(self, path_name):\n",
    "        \"\"\"\n",
    "        Save all data to a csv file, and returns the data as a Pandas DataFrame.\n",
    "        \n",
    "        ------------\n",
    "        Input(s):\n",
    "        path_name (str): the named path to store the csv file. E.g.: '/path1/path2/company.csv'\n",
    "        \n",
    "        Output(s):\n",
    "        df (pd.DataFrame): the scraped data in Pandas DataFrame format.\n",
    "        Also saves data to the path specified.\n",
    "        \n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(self.data)\n",
    "        df.to_csv(path_name, index=False, encoding='utf-8')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "class LinkedInJobScraper(BaseScraper):\n",
    "    \"\"\"\n",
    "    A custom LinkedIn Job Scraper object.\n",
    "    \n",
    "    ------------\n",
    "    Initialisation parameters:\n",
    "    total_jobs (int): the total number of jobs seen in the search.\n",
    "    raw_url (str): the raw url with style and ?start={} removed, for scraping. For example: \n",
    "        https://au.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=&location=Australia&locationId=&geoId=101452733&f_TPR=&f_C=2848\n",
    "    \n",
    "    ------------\n",
    "    Examples use of this object:\n",
    "    \n",
    "    - Without logging into LinkedIn, find jobs according to desired criteria on https://au.linkedin.com/jobs/search\n",
    "    - On Google Chrome, open the developers tool, then navigate to the `Network` tab.\n",
    "    - Scroll down on the jobs section, and as more jobs are loaded, look for an element appearing\n",
    "      on the left-hand panel of the developers tool starting with `search?`. Click on that element.\n",
    "    - Grab the Request URL on the right-hand panel of the developers tool. The URL looks like:\n",
    "      https://au.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?start=25\n",
    "    - Remove the `?start=25` at the end, and this will be the raw URL as input for the scraper.\n",
    "    - The total jobs appeared at the beginning of the search page is also to be specified,\n",
    "      to accurately calculate the number of loops to run during scrape().\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, total_jobs, *args, **kwargs):\n",
    "        super(LinkedInJobScraper, self).__init__(*args, **kwargs)\n",
    "        self.total_jobs = total_jobs\n",
    "     \n",
    "    def scrape(self, debug=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Scrape the specified URL.\n",
    "        \n",
    "        ------------\n",
    "        Input(s):\n",
    "        debug (bool, default=True): for debugging only, and should be ignored.\n",
    "        verbose (bool, default=True): whether to print progress for each individual job scraped.\n",
    "        \n",
    "        Output(s):\n",
    "        None. Writes data into the `data` attribute of the class.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Since each page contains 25 results, this is the total number of pages in the search\n",
    "        num_pages_in_search = math.ceil(self.total_jobs / 25)\n",
    "        \n",
    "        # Calculate the list of starting positions for job search, to iterate\n",
    "        iter_list = []\n",
    "        count_page = 0\n",
    "        while count_page < self.total_jobs:\n",
    "            iter_list.append(count_page)\n",
    "            count_page += 25\n",
    "        \n",
    "        # This logic ensures the calculation of the number of pages is correct\n",
    "        if debug:\n",
    "            assert num_pages_in_search == len(iter_list), \\\n",
    "            f'Mismatch between total pages in the search and total pages in iteration: \\nnum_pages_in_search = {num_pages_in_search}, len(iter_list) = {len(iter_list)}'\n",
    "        \n",
    "        # Get the scraping URL from the raw URL\n",
    "        url_scrape = \"{}&start={}\"\n",
    "        \n",
    "        # Initiate an empty job id list\n",
    "        job_id_list = []\n",
    "        \n",
    "        # For each page of the search, scrape all job IDs and append to the list\n",
    "        for i in iter_list:\n",
    "            # Request the website to scrape\n",
    "            res = requests.get(url_scrape.format(self.raw_url, i))\n",
    "\n",
    "            # Parse the html of that site\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            # Find all jobs on the page, which are in 'li' tags\n",
    "            jobs_on_this_page = soup.find_all('li')\n",
    "            \n",
    "            # For each jobs found, extract the job ID and append to job_id_list\n",
    "            for j in range(len(jobs_on_this_page)):\n",
    "                try:\n",
    "                    job_id = jobs_on_this_page[j].find(\"div\", {\"class\": \"base-card\"}).get('data-entity-urn').split(\":\")[3]\n",
    "                    job_id_list.append(job_id)\n",
    "                # If none found, continue search\n",
    "                except AttributeError as none:\n",
    "                    continue\n",
    "        \n",
    "        # Initiate a dictionary to store individual jobs scraped\n",
    "        data_individual = {}\n",
    "        \n",
    "        # Declare a shortened URL for jobs, which only differs in the job ID\n",
    "        url_job = 'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{}'\n",
    "\n",
    "        # For each job ID, append it to the url_job string, scrape job information, and store\n",
    "        for k in range(len(job_id_list)):\n",
    "            if verbose:\n",
    "                print(f'Scraping job {k+1} of {self.total_jobs}...')\n",
    "            \n",
    "            # request the URL\n",
    "            resp = requests.get(url_job.format(job_id_list[k]))\n",
    "            \n",
    "            # Scrape job contents\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            # Scrape company name\n",
    "            try:\n",
    "                data_individual[\"company\"] = soup.find(\"div\", {\"class\": \"top-card-layout__card\"}).find(\"a\").find(\"img\").get('alt')\n",
    "            except:\n",
    "                data_individual[\"company\"] = None\n",
    "            \n",
    "            # Scrape job titles\n",
    "            try:\n",
    "                data_individual[\"job_title\"] = soup.find(\"div\", {\"class\":\"top-card-layout__entity-info\"}).find(\"a\").text.strip()\n",
    "            except:\n",
    "                data_individual[\"job_title\"] = None\n",
    "\n",
    "            # Scrape seniority level\n",
    "            try:\n",
    "                data_individual[\"level\"] = soup.find(\"ul\", {\"class\": \"description__job-criteria-list\"}).find(\"li\").text.replace(\"Seniority level\",\"\").strip()\n",
    "            except:\n",
    "                data_individual[\"level\"] = None\n",
    "            \n",
    "            # Scrape job description\n",
    "            try:\n",
    "                data_individual[\"description\"] = soup.find(\"div\", {\"class\": \"show-more-less-html__markup\"}).text.strip()\n",
    "            except:\n",
    "                data_individual[\"description\"] = None\n",
    "            \n",
    "            # Append the job to the stored data attribute\n",
    "            self.data.append(data_individual)\n",
    "            \n",
    "            # Reset the individual data point to blank, ready for scraping the next job\n",
    "            data_individual = {}\n",
    "        \n",
    "        if verbose:\n",
    "            print('Done scraping all data.')\n",
    "            \n",
    "\n",
    "class SeekJobScraper(BaseScraper):\n",
    "    \n",
    "    def __init__(self, total_pages, *args, **kwargs):\n",
    "        super(SeekJobScraper, self).__init__(*args, **kwargs)\n",
    "        self.total_pages = total_pages\n",
    "        \n",
    "    def scrape(self, sleep_duration=3, verbose=True):\n",
    "        \"\"\"\n",
    "        Scrape the specified URL.\n",
    "        \n",
    "        ------------\n",
    "        Input(s):\n",
    "        sleep_duration (int, default=3): sleep duration for each GET request.\n",
    "        verbose (bool, default=True): whether to print progress for each individual job scraped.\n",
    "        \n",
    "        Output(s):\n",
    "        None. Writes data into the `data` attribute of the class.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Calculate the list of starting positions for job search, to iterate\n",
    "        iter_list = [p+1 for p in range(self.total_pages)]\n",
    "        \n",
    "        # Get the scraping URL from the raw URL\n",
    "        url_scrape = \"{}&page={}\"\n",
    "        \n",
    "        # Get the shortened job URL that only differs in the jobId at the end\n",
    "        url_job = 'https://www.seek.com.au/job/{}'\n",
    "    \n",
    "        # For each page of the search, scrape all job details and append to data storage attribute\n",
    "        for i in iter_list:\n",
    "            # Request the website to scrape\n",
    "            res = requests.get(url_scrape.format(self.raw_url, i))\n",
    "\n",
    "            # Parse the html of that site, and parse the json object as dict\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            json_data = json.loads(soup.text)['data']\n",
    "            num_jobs_on_this_page = len(json_data)\n",
    "\n",
    "            # Initiate an individual data point to attach to\n",
    "            data_individual = {}\n",
    "            \n",
    "            # For each job found, extract the job information\n",
    "            for j in range(len(json_data)):\n",
    "                try:\n",
    "                    # Only get organic job listing\n",
    "                    if json_data[j]['solMetadata']['jobAdType'] == 'ORGANIC':\n",
    "                        if verbose:\n",
    "                            print(f'Scraping job {j+1} of {num_jobs_on_this_page} on page {i}...')\n",
    "                        \n",
    "                        # Get company name\n",
    "                        data_individual['company'] = json_data[j]['companyName']\n",
    "                        \n",
    "                        # Get the job title\n",
    "                        data_individual['job_title'] = json_data[j]['title']\n",
    "                        \n",
    "                        # Get the job description by going into the job details page, using the shortened URL\n",
    "                        job_id = json_data[j]['solMetadata']['jobId']\n",
    "                        job_resp = requests.get(url_job.format(job_id))\n",
    "                        \n",
    "                        # Set sleep duration for each GET request, to allow for load time\n",
    "                        if j != len(json_data) - 1:\n",
    "                            sleep(sleep_duration)\n",
    "                        \n",
    "                        job_soup = BeautifulSoup(job_resp.text, 'html.parser')\n",
    "                        try:\n",
    "                            data_individual['description'] = job_soup.find('div', {'data-automation': 'jobAdDetails'}).find('div').text.strip()\n",
    "                        except:\n",
    "                            data_individual['description'] = None\n",
    "                        \n",
    "                        # Append the job details to the stored data attribute\n",
    "                        self.data.append(data_individual)\n",
    "\n",
    "                        # Reset the individual data point to blank, ready to scrape the next job\n",
    "                        data_individual = {}\n",
    "                \n",
    "                # If any of above is not found, continue search\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        if verbose:\n",
    "            print('Done scraping all data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e87610",
   "metadata": {},
   "source": [
    "# Scraping: BHP Ltd. on LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4be9bf31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 1...\n",
      "Scraping job 2...\n",
      "Scraping job 3...\n",
      "Scraping job 4...\n",
      "Scraping job 5...\n",
      "Scraping job 6...\n",
      "Scraping job 7...\n",
      "Scraping job 8...\n",
      "Scraping job 9...\n",
      "Scraping job 10...\n",
      "Scraping job 11...\n",
      "Scraping job 12...\n",
      "Scraping job 13...\n",
      "Scraping job 14...\n",
      "Scraping job 15...\n",
      "Scraping job 16...\n",
      "Scraping job 17...\n",
      "Scraping job 18...\n",
      "Scraping job 19...\n",
      "Scraping job 20...\n",
      "Scraping job 21...\n",
      "Scraping job 22...\n",
      "Scraping job 23...\n",
      "Scraping job 24...\n",
      "Scraping job 25...\n",
      "Scraping job 26...\n",
      "Scraping job 27...\n",
      "Scraping job 28...\n",
      "Scraping job 29...\n",
      "Scraping job 30...\n",
      "Scraping job 31...\n",
      "Scraping job 32...\n",
      "Scraping job 33...\n",
      "Scraping job 34...\n",
      "Scraping job 35...\n",
      "Scraping job 36...\n",
      "Scraping job 37...\n",
      "Scraping job 38...\n",
      "Scraping job 39...\n",
      "Scraping job 40...\n",
      "Scraping job 41...\n",
      "Scraping job 42...\n",
      "Scraping job 43...\n",
      "Scraping job 44...\n",
      "Scraping job 45...\n",
      "Scraping job 46...\n",
      "Scraping job 47...\n",
      "Scraping job 48...\n",
      "Scraping job 49...\n",
      "Scraping job 50...\n",
      "Scraping job 51...\n",
      "Scraping job 52...\n",
      "Scraping job 53...\n",
      "Scraping job 54...\n",
      "Scraping job 55...\n",
      "Scraping job 56...\n",
      "Scraping job 57...\n",
      "Scraping job 58...\n",
      "Scraping job 59...\n",
      "Scraping job 60...\n",
      "Scraping job 61...\n",
      "Scraping job 62...\n",
      "Scraping job 63...\n",
      "Scraping job 64...\n",
      "Scraping job 65...\n",
      "Scraping job 66...\n",
      "Scraping job 67...\n",
      "Scraping job 68...\n",
      "Scraping job 69...\n",
      "Scraping job 70...\n",
      "Scraping job 71...\n",
      "Scraping job 72...\n",
      "Scraping job 73...\n",
      "Scraping job 74...\n",
      "Scraping job 75...\n",
      "Scraping job 76...\n",
      "Scraping job 77...\n",
      "Scraping job 78...\n",
      "Scraping job 79...\n",
      "Scraping job 80...\n",
      "Scraping job 81...\n",
      "Scraping job 82...\n",
      "Scraping job 83...\n",
      "Scraping job 84...\n",
      "Scraping job 85...\n",
      "Scraping job 86...\n",
      "Scraping job 87...\n",
      "Scraping job 88...\n",
      "Scraping job 89...\n",
      "Scraping job 90...\n",
      "Scraping job 91...\n",
      "Scraping job 92...\n",
      "Scraping job 93...\n",
      "Scraping job 94...\n",
      "Scraping job 95...\n",
      "Scraping job 96...\n",
      "Scraping job 97...\n",
      "Scraping job 98...\n",
      "Scraping job 99...\n",
      "Scraping job 100...\n",
      "Scraping job 101...\n",
      "Scraping job 102...\n",
      "Scraping job 103...\n",
      "Scraping job 104...\n",
      "Scraping job 105...\n",
      "Scraping job 106...\n",
      "Scraping job 107...\n",
      "Scraping job 108...\n",
      "Scraping job 109...\n",
      "Scraping job 110...\n",
      "Scraping job 111...\n",
      "Scraping job 112...\n",
      "Scraping job 113...\n",
      "Scraping job 114...\n",
      "Scraping job 115...\n",
      "Scraping job 116...\n",
      "Scraping job 117...\n",
      "Scraping job 118...\n",
      "Scraping job 119...\n",
      "Scraping job 120...\n",
      "Scraping job 121...\n",
      "Scraping job 122...\n",
      "Scraping job 123...\n",
      "Scraping job 124...\n",
      "Scraping job 125...\n",
      "Scraping job 126...\n",
      "Scraping job 127...\n",
      "Scraping job 128...\n",
      "Scraping job 129...\n",
      "Scraping job 130...\n",
      "Scraping job 131...\n",
      "Scraping job 132...\n",
      "Scraping job 133...\n",
      "Scraping job 134...\n",
      "Scraping job 135...\n",
      "Scraping job 136...\n",
      "Scraping job 137...\n",
      "Scraping job 138...\n",
      "Scraping job 139...\n",
      "Scraping job 140...\n",
      "Scraping job 141...\n",
      "Scraping job 142...\n",
      "Scraping job 143...\n",
      "Scraping job 144...\n",
      "Scraping job 145...\n",
      "Scraping job 146...\n",
      "Scraping job 147...\n",
      "Done scraping all data.\n"
     ]
    }
   ],
   "source": [
    "bhp_scraper = LinkedInJobScraper(\n",
    "    147,\n",
    "    'https://au.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=&location=Australia&locationId=&geoId=101452733&f_TPR=&f_C=4509'\n",
    ")\n",
    "\n",
    "bhp_scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "925c581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhp_data = bhp_scraper.save_data('data/jobs_linkedin/bhp_jobs_linkedin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "27ffb96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>level</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BHP</td>\n",
       "      <td>Lead Project Delivery | Perth |</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>About BHPAt BHP we support our people to grow,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BHP</td>\n",
       "      <td>Specialist Data Science | Value Engineering | ...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>About BHPAt BHP we support our people to grow,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BHP</td>\n",
       "      <td>Principal Environment | Perth | Permanent</td>\n",
       "      <td>Director</td>\n",
       "      <td>About BHPAt BHP we support our people to grow,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BHP</td>\n",
       "      <td>Environmental Specialist | Nickel West | Kwina...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>About BHPAt BHP we support our people to grow,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BHP</td>\n",
       "      <td>Process Technician Entry | Nickel West | Kwinana</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>About BHPAt BHP we support our people to grow,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company                                          job_title  \\\n",
       "0     BHP                    Lead Project Delivery | Perth |   \n",
       "1     BHP  Specialist Data Science | Value Engineering | ...   \n",
       "2     BHP          Principal Environment | Perth | Permanent   \n",
       "3     BHP  Environmental Specialist | Nickel West | Kwina...   \n",
       "4     BHP   Process Technician Entry | Nickel West | Kwinana   \n",
       "\n",
       "              level                                        description  \n",
       "0  Mid-Senior level  About BHPAt BHP we support our people to grow,...  \n",
       "1       Entry level  About BHPAt BHP we support our people to grow,...  \n",
       "2          Director  About BHPAt BHP we support our people to grow,...  \n",
       "3       Entry level  About BHPAt BHP we support our people to grow,...  \n",
       "4       Entry level  About BHPAt BHP we support our people to grow,...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea71fb",
   "metadata": {},
   "source": [
    "# Scraping: Telstra Ltd. on LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc6002ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 1 of 80...\n",
      "Scraping job 2 of 80...\n",
      "Scraping job 3 of 80...\n",
      "Scraping job 4 of 80...\n",
      "Scraping job 5 of 80...\n",
      "Scraping job 6 of 80...\n",
      "Scraping job 7 of 80...\n",
      "Scraping job 8 of 80...\n",
      "Scraping job 9 of 80...\n",
      "Scraping job 10 of 80...\n",
      "Scraping job 11 of 80...\n",
      "Scraping job 12 of 80...\n",
      "Scraping job 13 of 80...\n",
      "Scraping job 14 of 80...\n",
      "Scraping job 15 of 80...\n",
      "Scraping job 16 of 80...\n",
      "Scraping job 17 of 80...\n",
      "Scraping job 18 of 80...\n",
      "Scraping job 19 of 80...\n",
      "Scraping job 20 of 80...\n",
      "Scraping job 21 of 80...\n",
      "Scraping job 22 of 80...\n",
      "Scraping job 23 of 80...\n",
      "Scraping job 24 of 80...\n",
      "Scraping job 25 of 80...\n",
      "Scraping job 26 of 80...\n",
      "Scraping job 27 of 80...\n",
      "Scraping job 28 of 80...\n",
      "Scraping job 29 of 80...\n",
      "Scraping job 30 of 80...\n",
      "Scraping job 31 of 80...\n",
      "Scraping job 32 of 80...\n",
      "Scraping job 33 of 80...\n",
      "Scraping job 34 of 80...\n",
      "Scraping job 35 of 80...\n",
      "Scraping job 36 of 80...\n",
      "Scraping job 37 of 80...\n",
      "Scraping job 38 of 80...\n",
      "Scraping job 39 of 80...\n",
      "Scraping job 40 of 80...\n",
      "Scraping job 41 of 80...\n",
      "Scraping job 42 of 80...\n",
      "Scraping job 43 of 80...\n",
      "Scraping job 44 of 80...\n",
      "Scraping job 45 of 80...\n",
      "Scraping job 46 of 80...\n",
      "Scraping job 47 of 80...\n",
      "Scraping job 48 of 80...\n",
      "Scraping job 49 of 80...\n",
      "Scraping job 50 of 80...\n",
      "Scraping job 51 of 80...\n",
      "Scraping job 52 of 80...\n",
      "Scraping job 53 of 80...\n",
      "Scraping job 54 of 80...\n",
      "Scraping job 55 of 80...\n",
      "Scraping job 56 of 80...\n",
      "Scraping job 57 of 80...\n",
      "Scraping job 58 of 80...\n",
      "Scraping job 59 of 80...\n",
      "Scraping job 60 of 80...\n",
      "Scraping job 61 of 80...\n",
      "Scraping job 62 of 80...\n",
      "Scraping job 63 of 80...\n",
      "Scraping job 64 of 80...\n",
      "Scraping job 65 of 80...\n",
      "Scraping job 66 of 80...\n",
      "Scraping job 67 of 80...\n",
      "Scraping job 68 of 80...\n",
      "Scraping job 69 of 80...\n",
      "Scraping job 70 of 80...\n",
      "Scraping job 71 of 80...\n",
      "Scraping job 72 of 80...\n",
      "Scraping job 73 of 80...\n",
      "Scraping job 74 of 80...\n",
      "Scraping job 75 of 80...\n",
      "Scraping job 76 of 80...\n",
      "Scraping job 77 of 80...\n",
      "Scraping job 78 of 80...\n",
      "Scraping job 79 of 80...\n",
      "Scraping job 80 of 80...\n",
      "Done scraping all data.\n"
     ]
    }
   ],
   "source": [
    "tls_scraper = LinkedInJobScraper(\n",
    "    total_jobs=80,\n",
    "    raw_url='https://au.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=&location=Australia&locationId=&geoId=101452733&f_TPR=&f_C=89885306%2C1635%2C122653%2C14383808%2C14455912%2C26547568%2C1636%2C3330130'\n",
    ")\n",
    "\n",
    "tls_scraper.scrape()\n",
    "\n",
    "tls_data = tls_scraper.save_data('data/jobs_linkedin/tls_jobs_linkedin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3d8b6363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>level</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Telstra Health</td>\n",
       "      <td>Product Manager</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Work options: HybridMedicalDirector is part of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Telstra Health</td>\n",
       "      <td>UX Designer</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Work options: HybridThe UX Designer needs to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Telstra</td>\n",
       "      <td>Customer Service and Sales Consultant</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Customer Service &amp; Sales Consultant At Telstra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Telstra Health</td>\n",
       "      <td>Service Desk Specialist</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Work options: Work From AnywhereTelstra Health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Telstra Health</td>\n",
       "      <td>Customer Success Manager</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>The Customer success Managers are trusted advi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company                              job_title             level  \\\n",
       "0  Telstra Health                        Product Manager       Entry level   \n",
       "1  Telstra Health                            UX Designer  Mid-Senior level   \n",
       "2         Telstra  Customer Service and Sales Consultant  Mid-Senior level   \n",
       "3  Telstra Health                Service Desk Specialist       Entry level   \n",
       "4  Telstra Health               Customer Success Manager  Mid-Senior level   \n",
       "\n",
       "                                         description  \n",
       "0  Work options: HybridMedicalDirector is part of...  \n",
       "1  Work options: HybridThe UX Designer needs to u...  \n",
       "2  Customer Service & Sales Consultant At Telstra...  \n",
       "3  Work options: Work From AnywhereTelstra Health...  \n",
       "4  The Customer success Managers are trusted advi...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c57097",
   "metadata": {},
   "source": [
    "# Scraping: Woolworths Group on LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0af3e201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 1 of 61...\n",
      "Scraping job 2 of 61...\n",
      "Scraping job 3 of 61...\n",
      "Scraping job 4 of 61...\n",
      "Scraping job 5 of 61...\n",
      "Scraping job 6 of 61...\n",
      "Scraping job 7 of 61...\n",
      "Scraping job 8 of 61...\n",
      "Scraping job 9 of 61...\n",
      "Scraping job 10 of 61...\n",
      "Scraping job 11 of 61...\n",
      "Scraping job 12 of 61...\n",
      "Scraping job 13 of 61...\n",
      "Scraping job 14 of 61...\n",
      "Scraping job 15 of 61...\n",
      "Scraping job 16 of 61...\n",
      "Scraping job 17 of 61...\n",
      "Scraping job 18 of 61...\n",
      "Scraping job 19 of 61...\n",
      "Scraping job 20 of 61...\n",
      "Scraping job 21 of 61...\n",
      "Scraping job 22 of 61...\n",
      "Scraping job 23 of 61...\n",
      "Scraping job 24 of 61...\n",
      "Scraping job 25 of 61...\n",
      "Scraping job 26 of 61...\n",
      "Scraping job 27 of 61...\n",
      "Scraping job 28 of 61...\n",
      "Scraping job 29 of 61...\n",
      "Scraping job 30 of 61...\n",
      "Scraping job 31 of 61...\n",
      "Scraping job 32 of 61...\n",
      "Scraping job 33 of 61...\n",
      "Scraping job 34 of 61...\n",
      "Scraping job 35 of 61...\n",
      "Scraping job 36 of 61...\n",
      "Scraping job 37 of 61...\n",
      "Scraping job 38 of 61...\n",
      "Scraping job 39 of 61...\n",
      "Scraping job 40 of 61...\n",
      "Scraping job 41 of 61...\n",
      "Scraping job 42 of 61...\n",
      "Scraping job 43 of 61...\n",
      "Scraping job 44 of 61...\n",
      "Scraping job 45 of 61...\n",
      "Scraping job 46 of 61...\n",
      "Scraping job 47 of 61...\n",
      "Scraping job 48 of 61...\n",
      "Scraping job 49 of 61...\n",
      "Scraping job 50 of 61...\n",
      "Scraping job 51 of 61...\n",
      "Scraping job 52 of 61...\n",
      "Scraping job 53 of 61...\n",
      "Scraping job 54 of 61...\n",
      "Scraping job 55 of 61...\n",
      "Scraping job 56 of 61...\n",
      "Scraping job 57 of 61...\n",
      "Scraping job 58 of 61...\n",
      "Scraping job 59 of 61...\n",
      "Scraping job 60 of 61...\n",
      "Scraping job 61 of 61...\n",
      "Done scraping all data.\n"
     ]
    }
   ],
   "source": [
    "wow_scraper = LinkedInJobScraper(\n",
    "    61,\n",
    "    'https://au.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=&location=Australia&locationId=&geoId=101452733&f_TPR=&f_C=295257'\n",
    ")\n",
    "\n",
    "wow_scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90ccb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wow_data = wow_scraper.save_data('data/jobs_linkedin/wow_jobs_linkedin.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef058a",
   "metadata": {},
   "source": [
    "# Full LinkedIn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9774bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wow = pd.read_csv('data/jobs_linkedin/wow_jobs_linkedin.csv')\n",
    "tls = pd.read_csv('data/jobs_linkedin/tls_jobs_linkedin.csv')\n",
    "cba = pd.read_csv('data/jobs_linkedin/cba_jobs_linkedin.csv')\n",
    "bhp = pd.read_csv('data/jobs_linkedin/bhp_jobs_linkedin.csv')\n",
    "csl = pd.read_csv('data/jobs_linkedin/csl_jobs_linkedin.csv')\n",
    "\n",
    "full_data = [wow, tls, cba, bhp, csl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ceb02fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>level</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woolworths Group</td>\n",
       "      <td>Data Engineering Lead</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>About Woolworths GroupWoolworths Group is a fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Woolworths Group</td>\n",
       "      <td>Account Executive - AGW Wholesale</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>About Woolworths GroupWoolworths Group is a fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Woolworths Group</td>\n",
       "      <td>HVAC &amp; R Technician</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>About Woolworths GroupWoolworths Group is a fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Woolworths Group</td>\n",
       "      <td>Analytics Engineering Manager</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Lead teams in the design, development, impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woolworths Group</td>\n",
       "      <td>IT Senior Business Analyst - Payments</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>About UsFounded within the Woolworths group, W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            company                              job_title             level  \\\n",
       "0  Woolworths Group                  Data Engineering Lead  Mid-Senior level   \n",
       "1  Woolworths Group      Account Executive - AGW Wholesale  Mid-Senior level   \n",
       "2  Woolworths Group                    HVAC & R Technician  Mid-Senior level   \n",
       "3  Woolworths Group          Analytics Engineering Manager  Mid-Senior level   \n",
       "4  Woolworths Group  IT Senior Business Analyst - Payments  Mid-Senior level   \n",
       "\n",
       "                                         description  \n",
       "0  About Woolworths GroupWoolworths Group is a fo...  \n",
       "1  About Woolworths GroupWoolworths Group is a fo...  \n",
       "2  About Woolworths GroupWoolworths Group is a fo...  \n",
       "3  Lead teams in the design, development, impleme...  \n",
       "4  About UsFounded within the Woolworths group, W...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.concat(full_data, ignore_index=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d774764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 612 entries, 0 to 611\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   company      601 non-null    object\n",
      " 1   job_title    601 non-null    object\n",
      " 2   level        601 non-null    object\n",
      " 3   description  601 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 19.2+ KB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5389caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>level</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    company job_title level description\n",
       "214     NaN       NaN   NaN         NaN\n",
       "215     NaN       NaN   NaN         NaN\n",
       "216     NaN       NaN   NaN         NaN\n",
       "241     NaN       NaN   NaN         NaN\n",
       "285     NaN       NaN   NaN         NaN\n",
       "286     NaN       NaN   NaN         NaN\n",
       "291     NaN       NaN   NaN         NaN\n",
       "294     NaN       NaN   NaN         NaN\n",
       "297     NaN       NaN   NaN         NaN\n",
       "345     NaN       NaN   NaN         NaN\n",
       "399     NaN       NaN   NaN         NaN"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df[full_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4c90a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d64b2180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 601 entries, 0 to 611\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   company      601 non-null    object\n",
      " 1   job_title    601 non-null    object\n",
      " 2   level        601 non-null    object\n",
      " 3   description  601 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 23.5+ KB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c913dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv('data/jobs_linkedin/all_jobs_linkedin.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d26a74",
   "metadata": {},
   "source": [
    "# Seek Job Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cb9e1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code\n",
    "\n",
    "# url3 = 'https://www.seek.com.au/api/chalice-search/v4/search?siteKey=AU-Main&sourcesystem=houston&userqueryid=13bc85bd693e885e2fae2bdbb068900b-7439281&userid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&usersessionid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&eventCaptureSessionId=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&where=All+Australia&seekSelectAllPages=true&companyname=Commonwealth+Bank+of+Australia&include=seodata&locale=en-AU&solId=8e8f007e-f45c-4c00-913d-7c862f50c2cb&page=1'\n",
    "# response = requests.get(url3)\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# site_json = json.loads(soup.text)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4e166305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'advertiser': {'id': '36143057',\n",
       "  'description': 'Commonwealth Bank - Business & Private Banking'},\n",
       " 'area': 'CBD, Inner West & Eastern Suburbs',\n",
       " 'areaId': 5027,\n",
       " 'areaWhereValue': 'Sydney CBD, Inner West & Eastern Suburbs Sydney NSW',\n",
       " 'automaticInclusion': False,\n",
       " 'branding': {'id': '8631e925-21b9-f44d-c462-d6f47f9a37dc.1',\n",
       "  'assets': {'logo': {'strategies': {'jdpLogo': 'https://bx-branding-gateway.cloud.seek.com.au/8631e925-21b9-f44d-c462-d6f47f9a37dc.1/jdpLogo',\n",
       "     'serpLogo': 'https://bx-branding-gateway.cloud.seek.com.au/8631e925-21b9-f44d-c462-d6f47f9a37dc.1/serpLogo'}}}},\n",
       " 'bulletPoints': ['Flexible working arrangements',\n",
       "  'Real career opportunities',\n",
       "  \"Work for Australia's largest bank\"],\n",
       " 'classification': {'id': '1204',\n",
       "  'description': 'Call Centre & Customer Service'},\n",
       " 'companyName': 'Commonwealth Bank of Australia',\n",
       " 'companyProfileStructuredDataId': 2034,\n",
       " 'displayStyle': {'search': 'A'},\n",
       " 'displayType': 'standout',\n",
       " 'listingDateDisplay': '2d ago',\n",
       " 'location': 'Sydney',\n",
       " 'locationId': 1000,\n",
       " 'locationWhereValue': 'All Sydney NSW',\n",
       " 'id': 67176650,\n",
       " 'isPremium': False,\n",
       " 'isStandOut': True,\n",
       " 'jobLocation': {'label': 'Eveleigh, Sydney NSW',\n",
       "  'countryCode': 'AU',\n",
       "  'seoHierarchy': [{'contextualName': 'Eveleigh NSW 2015'},\n",
       "   {'contextualName': 'All Sydney NSW'}]},\n",
       " 'listingDate': '2023-05-02T00:27:58Z',\n",
       " 'logo': {'id': '', 'description': None},\n",
       " 'roleId': 'Banking-Customer-Service-Representative',\n",
       " 'salary': '',\n",
       " 'solMetadata': {'searchRequestToken': '72d6c46a-f2e7-44da-a96f-a980b3a9c976',\n",
       "  'token': '0~72d6c46a-f2e7-44da-a96f-a980b3a9c976',\n",
       "  'jobId': '67176650',\n",
       "  'section': 'MAIN',\n",
       "  'sectionRank': 1,\n",
       "  'jobAdType': 'ORGANIC',\n",
       "  'tags': {'mordor__flights': 'mordor_128', 'mordor__s': '0'}},\n",
       " 'subClassification': {'id': '6085',\n",
       "  'description': 'Customer Service - Call Centre'},\n",
       " 'suburb': 'Eveleigh',\n",
       " 'suburbId': 20203,\n",
       " 'suburbWhereValue': 'Eveleigh NSW 2015',\n",
       " 'teaser': \"The Business Banking Contact Centre supports a range of business customer's financial needs to move their business from today into tomorrow.\",\n",
       " 'title': 'Business Banking Customer Service Representative',\n",
       " 'tracking': 'ewogICJ0b2tlbiI6ICJhYWU1ZjQ0MC0zMmE1LTQwNzMtODEyZC1mMDdhMDM5NTM0ODVfMSIKfQ==',\n",
       " 'workType': 'Full Time',\n",
       " 'isPrivateAdvertiser': False}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test code\n",
    "\n",
    "# site_json[0]\n",
    "\n",
    "## -------------------------------------------\n",
    "## Important information:\n",
    "## - data is in json['data']\n",
    "## - 'jobAdType' in 'solMetadata' should be 'ORGANIC'\n",
    "## - Get 'jobId' from 'solMetadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "be592378",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test code\n",
    "\n",
    "# test_data = {}\n",
    "# test_url_job = 'https://www.seek.com.au/job/67176650'\n",
    "\n",
    "# job_resp = requests.get(test_url_job)\n",
    "# job_soup = BeautifulSoup(job_resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "94034844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Work from home options availableStructured opportunity to developWork for Australia's largest bank Do work that matters The Business Banking Contact Centre supports a range of business customer's financial needs to move their business from today into tomorrow. Our Business Banking team provide invaluable support to our frontline Relationship Managers as the first point of contact to our business and private banking customers.  See yourself in our team  You'll be joining the Business Banking team - a hardworking bunch who support our online and phone business banking activities. Customer experience will be at the centre of everything you do. As a Business Banking Associate you will receive incoming calls and provide a premium level of technical support over the phone to assist our clients with their Business Banking and their merchant facilities.  What we are looking for  Previous customer service experience and exceptional customer service skills is essentialFinance experience or an interest in banking is desirableSomeone who thrives in a busy environmentHaving a resilient nature is essential in this roleThe ability to master different technologies and multitask Roster Our customers are at the centre of everything we do. To be there for them every step of the way we'll need you to commit to a rotating roster which operates between the hours of:  Monday-Sunday7.00am-10.00pmYou will need to be available to work weekends on a regular bases (every 3-6 weeks) Training commences in June 2023 with all new starters undergoing 4 weeks of mandatory full time training, Monday to Friday 9-5pm.  *Please note we cannot provide any time off during the 4 weeks training, so if you can have any upcoming leave during the training period, we recommend you apply for the next campaign later in the year.   Where will you go?   This role is something you can really sink your teeth into, and most people stay in the role for over 12 months. If you live the values and demonstrate the people capabilities we can offer great opportunities both within the call centre environment or beyond in the wider group. We'll help you find the next step that's right for you and enable you to be the best you can be. Whether you're passionate about customer service, driven by data, or called by creativity, a career here is for you. Our people bring their diverse backgrounds and unique perspectives to build a respectful, inclusive and flexible workplace. One where we're driven by our values, and supported to share ideas, initiatives, and energy. One where making a positive impact for customers, communities and each other is part of our every day.Here, you'll thrive. You'll be supported when faced with challenges, and empowered to tackle new opportunities. We really love working here, and we think you will too.We're determined to make a real difference for Australia's first peoples. We encourage all interested applicants to apply. If you're already part of the Commonwealth Bank Group (including Bankwest), you'll need to apply through Sidekick [link removed] to submit a valid application. We're keen to support you with the next step in your career.\"}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test code\n",
    "\n",
    "# try:\n",
    "#     test_data['description'] = job_soup.find('div', {'data-automation': 'jobAdDetails'}).find('div').text.strip()\n",
    "# except:\n",
    "#     test_data['description'] = None\n",
    "\n",
    "# test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9af5b2",
   "metadata": {},
   "source": [
    "# Scraping: Commonwealth Bank jobs on Seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "acd7f5b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 1 of 20 on page 1...\n",
      "Scraping job 2 of 20 on page 1...\n",
      "Scraping job 3 of 20 on page 1...\n",
      "Scraping job 4 of 20 on page 1...\n",
      "Scraping job 5 of 20 on page 1...\n",
      "Scraping job 6 of 20 on page 1...\n",
      "Scraping job 7 of 20 on page 1...\n",
      "Scraping job 8 of 20 on page 1...\n",
      "Scraping job 9 of 20 on page 1...\n",
      "Scraping job 10 of 20 on page 1...\n",
      "Scraping job 11 of 20 on page 1...\n",
      "Scraping job 12 of 20 on page 1...\n",
      "Scraping job 13 of 20 on page 1...\n",
      "Scraping job 14 of 20 on page 1...\n",
      "Scraping job 15 of 20 on page 1...\n",
      "Scraping job 16 of 20 on page 1...\n",
      "Scraping job 17 of 20 on page 1...\n",
      "Scraping job 18 of 20 on page 1...\n",
      "Scraping job 19 of 20 on page 1...\n",
      "Scraping job 20 of 20 on page 1...\n",
      "Scraping job 1 of 20 on page 2...\n",
      "Scraping job 2 of 20 on page 2...\n",
      "Scraping job 3 of 20 on page 2...\n",
      "Scraping job 4 of 20 on page 2...\n",
      "Scraping job 5 of 20 on page 2...\n",
      "Scraping job 6 of 20 on page 2...\n",
      "Scraping job 7 of 20 on page 2...\n",
      "Scraping job 8 of 20 on page 2...\n",
      "Scraping job 9 of 20 on page 2...\n",
      "Scraping job 10 of 20 on page 2...\n",
      "Scraping job 11 of 20 on page 2...\n",
      "Scraping job 12 of 20 on page 2...\n",
      "Scraping job 13 of 20 on page 2...\n",
      "Scraping job 14 of 20 on page 2...\n",
      "Scraping job 15 of 20 on page 2...\n",
      "Scraping job 16 of 20 on page 2...\n",
      "Scraping job 17 of 20 on page 2...\n",
      "Scraping job 18 of 20 on page 2...\n",
      "Scraping job 19 of 20 on page 2...\n",
      "Scraping job 20 of 20 on page 2...\n",
      "Scraping job 1 of 20 on page 3...\n",
      "Scraping job 2 of 20 on page 3...\n",
      "Scraping job 3 of 20 on page 3...\n",
      "Scraping job 4 of 20 on page 3...\n",
      "Scraping job 5 of 20 on page 3...\n",
      "Scraping job 6 of 20 on page 3...\n",
      "Scraping job 7 of 20 on page 3...\n",
      "Scraping job 8 of 20 on page 3...\n",
      "Scraping job 9 of 20 on page 3...\n",
      "Scraping job 10 of 20 on page 3...\n",
      "Scraping job 11 of 20 on page 3...\n",
      "Scraping job 12 of 20 on page 3...\n",
      "Scraping job 13 of 20 on page 3...\n",
      "Scraping job 14 of 20 on page 3...\n",
      "Scraping job 15 of 20 on page 3...\n",
      "Scraping job 16 of 20 on page 3...\n",
      "Scraping job 17 of 20 on page 3...\n",
      "Scraping job 18 of 20 on page 3...\n",
      "Scraping job 19 of 20 on page 3...\n",
      "Scraping job 20 of 20 on page 3...\n",
      "Scraping job 1 of 20 on page 4...\n",
      "Scraping job 2 of 20 on page 4...\n",
      "Scraping job 3 of 20 on page 4...\n",
      "Scraping job 4 of 20 on page 4...\n",
      "Scraping job 5 of 20 on page 4...\n",
      "Scraping job 6 of 20 on page 4...\n",
      "Scraping job 7 of 20 on page 4...\n",
      "Scraping job 8 of 20 on page 4...\n",
      "Scraping job 9 of 20 on page 4...\n",
      "Scraping job 10 of 20 on page 4...\n",
      "Scraping job 11 of 20 on page 4...\n",
      "Scraping job 12 of 20 on page 4...\n",
      "Scraping job 13 of 20 on page 4...\n",
      "Scraping job 14 of 20 on page 4...\n",
      "Scraping job 15 of 20 on page 4...\n",
      "Scraping job 16 of 20 on page 4...\n",
      "Scraping job 17 of 20 on page 4...\n",
      "Scraping job 18 of 20 on page 4...\n",
      "Scraping job 19 of 20 on page 4...\n",
      "Scraping job 20 of 20 on page 4...\n",
      "Scraping job 1 of 20 on page 5...\n",
      "Scraping job 2 of 20 on page 5...\n",
      "Scraping job 3 of 20 on page 5...\n",
      "Scraping job 4 of 20 on page 5...\n",
      "Scraping job 5 of 20 on page 5...\n",
      "Scraping job 6 of 20 on page 5...\n",
      "Scraping job 7 of 20 on page 5...\n",
      "Scraping job 8 of 20 on page 5...\n",
      "Scraping job 9 of 20 on page 5...\n",
      "Scraping job 10 of 20 on page 5...\n",
      "Scraping job 11 of 20 on page 5...\n",
      "Scraping job 12 of 20 on page 5...\n",
      "Scraping job 13 of 20 on page 5...\n",
      "Scraping job 14 of 20 on page 5...\n",
      "Scraping job 15 of 20 on page 5...\n",
      "Scraping job 16 of 20 on page 5...\n",
      "Scraping job 17 of 20 on page 5...\n",
      "Scraping job 18 of 20 on page 5...\n",
      "Scraping job 19 of 20 on page 5...\n",
      "Scraping job 20 of 20 on page 5...\n",
      "Scraping job 1 of 20 on page 6...\n",
      "Scraping job 2 of 20 on page 6...\n",
      "Scraping job 3 of 20 on page 6...\n",
      "Scraping job 4 of 20 on page 6...\n",
      "Scraping job 5 of 20 on page 6...\n",
      "Scraping job 6 of 20 on page 6...\n",
      "Scraping job 7 of 20 on page 6...\n",
      "Scraping job 8 of 20 on page 6...\n",
      "Scraping job 9 of 20 on page 6...\n",
      "Scraping job 10 of 20 on page 6...\n",
      "Scraping job 11 of 20 on page 6...\n",
      "Scraping job 12 of 20 on page 6...\n",
      "Scraping job 13 of 20 on page 6...\n",
      "Scraping job 14 of 20 on page 6...\n",
      "Scraping job 15 of 20 on page 6...\n",
      "Scraping job 16 of 20 on page 6...\n",
      "Scraping job 17 of 20 on page 6...\n",
      "Scraping job 18 of 20 on page 6...\n",
      "Scraping job 19 of 20 on page 6...\n",
      "Scraping job 20 of 20 on page 6...\n",
      "Scraping job 1 of 20 on page 7...\n",
      "Scraping job 2 of 20 on page 7...\n",
      "Scraping job 3 of 20 on page 7...\n",
      "Scraping job 4 of 20 on page 7...\n",
      "Scraping job 5 of 20 on page 7...\n",
      "Scraping job 6 of 20 on page 7...\n",
      "Scraping job 7 of 20 on page 7...\n",
      "Scraping job 8 of 20 on page 7...\n",
      "Scraping job 9 of 20 on page 7...\n",
      "Scraping job 10 of 20 on page 7...\n",
      "Scraping job 11 of 20 on page 7...\n",
      "Scraping job 12 of 20 on page 7...\n",
      "Scraping job 13 of 20 on page 7...\n",
      "Scraping job 14 of 20 on page 7...\n",
      "Scraping job 15 of 20 on page 7...\n",
      "Scraping job 16 of 20 on page 7...\n",
      "Scraping job 17 of 20 on page 7...\n",
      "Scraping job 18 of 20 on page 7...\n",
      "Scraping job 19 of 20 on page 7...\n",
      "Scraping job 20 of 20 on page 7...\n",
      "Scraping job 1 of 20 on page 8...\n",
      "Scraping job 2 of 20 on page 8...\n",
      "Scraping job 3 of 20 on page 8...\n",
      "Scraping job 4 of 20 on page 8...\n",
      "Scraping job 5 of 20 on page 8...\n",
      "Scraping job 6 of 20 on page 8...\n",
      "Scraping job 7 of 20 on page 8...\n",
      "Scraping job 8 of 20 on page 8...\n",
      "Scraping job 9 of 20 on page 8...\n",
      "Scraping job 10 of 20 on page 8...\n",
      "Scraping job 11 of 20 on page 8...\n",
      "Scraping job 12 of 20 on page 8...\n",
      "Scraping job 13 of 20 on page 8...\n",
      "Scraping job 14 of 20 on page 8...\n",
      "Scraping job 15 of 20 on page 8...\n",
      "Scraping job 16 of 20 on page 8...\n",
      "Scraping job 17 of 20 on page 8...\n",
      "Scraping job 18 of 20 on page 8...\n",
      "Scraping job 19 of 20 on page 8...\n",
      "Scraping job 20 of 20 on page 8...\n",
      "Scraping job 1 of 20 on page 9...\n",
      "Scraping job 2 of 20 on page 9...\n",
      "Scraping job 3 of 20 on page 9...\n",
      "Scraping job 4 of 20 on page 9...\n",
      "Scraping job 5 of 20 on page 9...\n",
      "Scraping job 6 of 20 on page 9...\n",
      "Scraping job 7 of 20 on page 9...\n",
      "Scraping job 8 of 20 on page 9...\n",
      "Scraping job 9 of 20 on page 9...\n",
      "Scraping job 10 of 20 on page 9...\n",
      "Scraping job 11 of 20 on page 9...\n",
      "Scraping job 12 of 20 on page 9...\n",
      "Scraping job 13 of 20 on page 9...\n",
      "Scraping job 14 of 20 on page 9...\n",
      "Scraping job 15 of 20 on page 9...\n",
      "Scraping job 16 of 20 on page 9...\n",
      "Scraping job 17 of 20 on page 9...\n",
      "Scraping job 18 of 20 on page 9...\n",
      "Scraping job 19 of 20 on page 9...\n",
      "Scraping job 20 of 20 on page 9...\n",
      "Scraping job 1 of 17 on page 10...\n",
      "Scraping job 2 of 17 on page 10...\n",
      "Scraping job 3 of 17 on page 10...\n",
      "Scraping job 4 of 17 on page 10...\n",
      "Scraping job 5 of 17 on page 10...\n",
      "Scraping job 6 of 17 on page 10...\n",
      "Scraping job 7 of 17 on page 10...\n",
      "Scraping job 8 of 17 on page 10...\n",
      "Scraping job 9 of 17 on page 10...\n",
      "Scraping job 10 of 17 on page 10...\n",
      "Scraping job 11 of 17 on page 10...\n",
      "Scraping job 12 of 17 on page 10...\n",
      "Scraping job 13 of 17 on page 10...\n",
      "Scraping job 14 of 17 on page 10...\n",
      "Scraping job 15 of 17 on page 10...\n",
      "Scraping job 16 of 17 on page 10...\n",
      "Scraping job 17 of 17 on page 10...\n",
      "Done scraping all data.\n"
     ]
    }
   ],
   "source": [
    "cba_seek_scraper = SeekJobScraper(\n",
    "    total_pages=10,\n",
    "    raw_url='https://www.seek.com.au/api/chalice-search/v4/search?siteKey=AU-Main&sourcesystem=houston&userqueryid=13bc85bd693e885e2fae2bdbb068900b-7439281&userid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&usersessionid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&eventCaptureSessionId=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&where=All+Australia&seekSelectAllPages=true&companyname=Commonwealth+Bank+of+Australia&include=seodata&locale=en-AU&solId=8e8f007e-f45c-4c00-913d-7c862f50c2cb'\n",
    ")\n",
    "\n",
    "cba_seek_scraper.scrape()\n",
    "\n",
    "cba_seek_jobs = cba_seek_scraper.save_data('data/jobs_seek/cba_jobs_seek.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "add70b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [company, job_title, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_null_rows(cba_seek_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819ece4",
   "metadata": {},
   "source": [
    "# Scraping: CSL jobs on Seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e69806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 1 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 2 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 3 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 4 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 5 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 6 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 7 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 8 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 9 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 10 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 11 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 12 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 13 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 14 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 15 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 16 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 17 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 18 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 19 of 20 on page 1...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 20 of 20 on page 1...\n",
      "Scraping job 1 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 2 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 3 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 4 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 5 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 6 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 7 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 8 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 9 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 10 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 11 of 12 on page 2...\n",
      "Sleeping for 3 seconds...\n",
      "Scraping job 12 of 12 on page 2...\n",
      "Done scraping all data.\n"
     ]
    }
   ],
   "source": [
    "csl_seek_scraper = SeekJobScraper(\n",
    "    total_pages=2,\n",
    "    raw_url='https://www.seek.com.au/api/chalice-search/v4/search?siteKey=AU-Main&sourcesystem=houston&userqueryid=1b2af659e84347999573e079fb6fbfbc-4136558&userid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&usersessionid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&eventCaptureSessionId=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&where=All+Australia&seekSelectAllPages=true&companyname=CSL+Limited&include=seodata&locale=en-AU&solId=8e8f007e-f45c-4c00-913d-7c862f50c2cb'\n",
    ")\n",
    "\n",
    "csl_seek_scraper.scrape()\n",
    "\n",
    "csl_seek_jobs = csl_seek_scraper.save_data('data/jobs_seek/csl_jobs_seek.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51aeebac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [company, job_title, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_null_rows(csl_seek_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896971d",
   "metadata": {},
   "source": [
    "# Scraping: BHP jobs on Seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fbc4a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 1 of 20 on page 1...\n",
      "Scraping job 2 of 20 on page 1...\n",
      "Scraping job 3 of 20 on page 1...\n",
      "Scraping job 4 of 20 on page 1...\n",
      "Scraping job 5 of 20 on page 1...\n",
      "Scraping job 6 of 20 on page 1...\n",
      "Scraping job 7 of 20 on page 1...\n",
      "Scraping job 8 of 20 on page 1...\n",
      "Scraping job 9 of 20 on page 1...\n",
      "Scraping job 10 of 20 on page 1...\n",
      "Scraping job 11 of 20 on page 1...\n",
      "Scraping job 12 of 20 on page 1...\n",
      "Scraping job 13 of 20 on page 1...\n",
      "Scraping job 14 of 20 on page 1...\n",
      "Scraping job 15 of 20 on page 1...\n",
      "Scraping job 16 of 20 on page 1...\n",
      "Scraping job 17 of 20 on page 1...\n",
      "Scraping job 18 of 20 on page 1...\n",
      "Scraping job 19 of 20 on page 1...\n",
      "Scraping job 20 of 20 on page 1...\n",
      "Scraping job 1 of 20 on page 2...\n",
      "Scraping job 2 of 20 on page 2...\n",
      "Scraping job 3 of 20 on page 2...\n",
      "Scraping job 4 of 20 on page 2...\n",
      "Scraping job 5 of 20 on page 2...\n",
      "Scraping job 6 of 20 on page 2...\n",
      "Scraping job 7 of 20 on page 2...\n",
      "Scraping job 8 of 20 on page 2...\n",
      "Scraping job 9 of 20 on page 2...\n",
      "Scraping job 10 of 20 on page 2...\n",
      "Scraping job 11 of 20 on page 2...\n",
      "Scraping job 12 of 20 on page 2...\n",
      "Scraping job 13 of 20 on page 2...\n",
      "Scraping job 14 of 20 on page 2...\n",
      "Scraping job 15 of 20 on page 2...\n",
      "Scraping job 16 of 20 on page 2...\n",
      "Scraping job 17 of 20 on page 2...\n",
      "Scraping job 18 of 20 on page 2...\n",
      "Scraping job 19 of 20 on page 2...\n",
      "Scraping job 20 of 20 on page 2...\n",
      "Scraping job 1 of 20 on page 3...\n",
      "Scraping job 2 of 20 on page 3...\n",
      "Scraping job 3 of 20 on page 3...\n",
      "Scraping job 4 of 20 on page 3...\n",
      "Scraping job 5 of 20 on page 3...\n",
      "Scraping job 6 of 20 on page 3...\n",
      "Scraping job 7 of 20 on page 3...\n",
      "Scraping job 8 of 20 on page 3...\n",
      "Scraping job 9 of 20 on page 3...\n",
      "Scraping job 10 of 20 on page 3...\n",
      "Scraping job 11 of 20 on page 3...\n",
      "Scraping job 12 of 20 on page 3...\n",
      "Scraping job 13 of 20 on page 3...\n",
      "Scraping job 14 of 20 on page 3...\n",
      "Scraping job 15 of 20 on page 3...\n",
      "Scraping job 16 of 20 on page 3...\n",
      "Scraping job 17 of 20 on page 3...\n",
      "Scraping job 18 of 20 on page 3...\n",
      "Scraping job 19 of 20 on page 3...\n",
      "Scraping job 20 of 20 on page 3...\n",
      "Scraping job 1 of 20 on page 4...\n",
      "Scraping job 2 of 20 on page 4...\n",
      "Scraping job 3 of 20 on page 4...\n",
      "Scraping job 4 of 20 on page 4...\n",
      "Scraping job 5 of 20 on page 4...\n",
      "Scraping job 6 of 20 on page 4...\n",
      "Scraping job 7 of 20 on page 4...\n",
      "Scraping job 8 of 20 on page 4...\n",
      "Scraping job 9 of 20 on page 4...\n",
      "Scraping job 10 of 20 on page 4...\n",
      "Scraping job 11 of 20 on page 4...\n",
      "Scraping job 12 of 20 on page 4...\n",
      "Scraping job 13 of 20 on page 4...\n",
      "Scraping job 14 of 20 on page 4...\n",
      "Scraping job 15 of 20 on page 4...\n",
      "Scraping job 16 of 20 on page 4...\n",
      "Scraping job 17 of 20 on page 4...\n",
      "Scraping job 18 of 20 on page 4...\n",
      "Scraping job 19 of 20 on page 4...\n",
      "Scraping job 20 of 20 on page 4...\n",
      "Scraping job 1 of 20 on page 5...\n",
      "Scraping job 2 of 20 on page 5...\n",
      "Scraping job 3 of 20 on page 5...\n",
      "Scraping job 4 of 20 on page 5...\n",
      "Scraping job 5 of 20 on page 5...\n",
      "Scraping job 6 of 20 on page 5...\n",
      "Scraping job 7 of 20 on page 5...\n",
      "Scraping job 8 of 20 on page 5...\n",
      "Scraping job 9 of 20 on page 5...\n",
      "Scraping job 10 of 20 on page 5...\n",
      "Scraping job 11 of 20 on page 5...\n",
      "Scraping job 12 of 20 on page 5...\n",
      "Scraping job 13 of 20 on page 5...\n",
      "Scraping job 14 of 20 on page 5...\n",
      "Scraping job 15 of 20 on page 5...\n",
      "Scraping job 16 of 20 on page 5...\n",
      "Scraping job 17 of 20 on page 5...\n",
      "Scraping job 18 of 20 on page 5...\n",
      "Scraping job 19 of 20 on page 5...\n",
      "Scraping job 20 of 20 on page 5...\n",
      "Scraping job 1 of 20 on page 6...\n",
      "Scraping job 2 of 20 on page 6...\n",
      "Scraping job 3 of 20 on page 6...\n",
      "Scraping job 4 of 20 on page 6...\n",
      "Scraping job 5 of 20 on page 6...\n",
      "Scraping job 6 of 20 on page 6...\n",
      "Scraping job 7 of 20 on page 6...\n",
      "Scraping job 8 of 20 on page 6...\n",
      "Scraping job 9 of 20 on page 6...\n",
      "Scraping job 10 of 20 on page 6...\n",
      "Scraping job 11 of 20 on page 6...\n",
      "Scraping job 12 of 20 on page 6...\n",
      "Scraping job 13 of 20 on page 6...\n",
      "Scraping job 14 of 20 on page 6...\n",
      "Scraping job 15 of 20 on page 6...\n",
      "Scraping job 16 of 20 on page 6...\n",
      "Scraping job 17 of 20 on page 6...\n",
      "Scraping job 18 of 20 on page 6...\n",
      "Scraping job 19 of 20 on page 6...\n",
      "Scraping job 20 of 20 on page 6...\n",
      "Scraping job 1 of 20 on page 7...\n",
      "Scraping job 2 of 20 on page 7...\n",
      "Scraping job 3 of 20 on page 7...\n",
      "Scraping job 4 of 20 on page 7...\n",
      "Scraping job 5 of 20 on page 7...\n",
      "Scraping job 6 of 20 on page 7...\n",
      "Scraping job 7 of 20 on page 7...\n",
      "Scraping job 8 of 20 on page 7...\n",
      "Scraping job 9 of 20 on page 7...\n",
      "Scraping job 10 of 20 on page 7...\n",
      "Scraping job 11 of 20 on page 7...\n",
      "Scraping job 12 of 20 on page 7...\n",
      "Scraping job 13 of 20 on page 7...\n",
      "Scraping job 14 of 20 on page 7...\n",
      "Scraping job 15 of 20 on page 7...\n",
      "Scraping job 16 of 20 on page 7...\n",
      "Scraping job 17 of 20 on page 7...\n",
      "Scraping job 18 of 20 on page 7...\n",
      "Scraping job 19 of 20 on page 7...\n",
      "Scraping job 20 of 20 on page 7...\n",
      "Scraping job 1 of 14 on page 8...\n",
      "Scraping job 2 of 14 on page 8...\n",
      "Scraping job 3 of 14 on page 8...\n",
      "Scraping job 4 of 14 on page 8...\n",
      "Scraping job 5 of 14 on page 8...\n",
      "Scraping job 6 of 14 on page 8...\n",
      "Scraping job 7 of 14 on page 8...\n",
      "Scraping job 8 of 14 on page 8...\n",
      "Scraping job 9 of 14 on page 8...\n",
      "Scraping job 10 of 14 on page 8...\n",
      "Scraping job 11 of 14 on page 8...\n",
      "Scraping job 12 of 14 on page 8...\n",
      "Scraping job 13 of 14 on page 8...\n",
      "Scraping job 14 of 14 on page 8...\n",
      "Done scraping all data.\n"
     ]
    }
   ],
   "source": [
    "bhp_seek_scraper = SeekJobScraper(\n",
    "    total_pages=8,\n",
    "    raw_url='https://www.seek.com.au/api/chalice-search/v4/search?siteKey=AU-Main&sourcesystem=houston&userqueryid=f16954fddb5e149bc48c68471d640531-4750155&userid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&usersessionid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&eventCaptureSessionId=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&where=All+Australia&seekSelectAllPages=true&companyname=BHP&include=seodata&locale=en-AU&solId=8e8f007e-f45c-4c00-913d-7c862f50c2cb'\n",
    ")\n",
    "\n",
    "bhp_seek_scraper.scrape()\n",
    "\n",
    "bhp_seek_jobs = bhp_seek_scraper.save_data('data/jobs_seek/bhp_jobs_seek.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f86669d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [company, job_title, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_null_rows(bhp_seek_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000d0b6",
   "metadata": {},
   "source": [
    "# Scraping: Woolworths Group jobs on Seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ff3ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 1 of 20 on page 1...\n",
      "Scraping job 2 of 20 on page 1...\n",
      "Scraping job 3 of 20 on page 1...\n",
      "Scraping job 4 of 20 on page 1...\n",
      "Scraping job 5 of 20 on page 1...\n",
      "Scraping job 6 of 20 on page 1...\n",
      "Scraping job 7 of 20 on page 1...\n",
      "Scraping job 8 of 20 on page 1...\n",
      "Scraping job 9 of 20 on page 1...\n",
      "Scraping job 10 of 20 on page 1...\n",
      "Scraping job 11 of 20 on page 1...\n",
      "Scraping job 12 of 20 on page 1...\n",
      "Scraping job 13 of 20 on page 1...\n",
      "Scraping job 14 of 20 on page 1...\n",
      "Scraping job 15 of 20 on page 1...\n",
      "Scraping job 16 of 20 on page 1...\n",
      "Scraping job 17 of 20 on page 1...\n",
      "Scraping job 18 of 20 on page 1...\n",
      "Scraping job 19 of 20 on page 1...\n",
      "Scraping job 20 of 20 on page 1...\n",
      "Scraping job 1 of 20 on page 2...\n",
      "Scraping job 2 of 20 on page 2...\n",
      "Scraping job 3 of 20 on page 2...\n",
      "Scraping job 4 of 20 on page 2...\n",
      "Scraping job 5 of 20 on page 2...\n",
      "Scraping job 6 of 20 on page 2...\n",
      "Scraping job 7 of 20 on page 2...\n",
      "Scraping job 8 of 20 on page 2...\n",
      "Scraping job 9 of 20 on page 2...\n",
      "Scraping job 10 of 20 on page 2...\n",
      "Scraping job 11 of 20 on page 2...\n",
      "Scraping job 12 of 20 on page 2...\n",
      "Scraping job 13 of 20 on page 2...\n",
      "Scraping job 14 of 20 on page 2...\n",
      "Scraping job 15 of 20 on page 2...\n",
      "Scraping job 16 of 20 on page 2...\n",
      "Scraping job 17 of 20 on page 2...\n",
      "Scraping job 18 of 20 on page 2...\n",
      "Scraping job 19 of 20 on page 2...\n",
      "Scraping job 20 of 20 on page 2...\n",
      "Scraping job 1 of 20 on page 3...\n",
      "Scraping job 2 of 20 on page 3...\n",
      "Scraping job 3 of 20 on page 3...\n",
      "Scraping job 4 of 20 on page 3...\n",
      "Scraping job 5 of 20 on page 3...\n",
      "Scraping job 6 of 20 on page 3...\n",
      "Scraping job 7 of 20 on page 3...\n",
      "Scraping job 8 of 20 on page 3...\n",
      "Scraping job 9 of 20 on page 3...\n",
      "Scraping job 10 of 20 on page 3...\n",
      "Scraping job 11 of 20 on page 3...\n",
      "Scraping job 12 of 20 on page 3...\n",
      "Scraping job 13 of 20 on page 3...\n",
      "Scraping job 14 of 20 on page 3...\n",
      "Scraping job 15 of 20 on page 3...\n",
      "Scraping job 16 of 20 on page 3...\n",
      "Scraping job 17 of 20 on page 3...\n",
      "Scraping job 18 of 20 on page 3...\n",
      "Scraping job 19 of 20 on page 3...\n",
      "Scraping job 20 of 20 on page 3...\n",
      "Scraping job 1 of 20 on page 4...\n",
      "Scraping job 2 of 20 on page 4...\n",
      "Scraping job 3 of 20 on page 4...\n",
      "Scraping job 4 of 20 on page 4...\n",
      "Scraping job 5 of 20 on page 4...\n",
      "Scraping job 6 of 20 on page 4...\n",
      "Scraping job 7 of 20 on page 4...\n",
      "Scraping job 8 of 20 on page 4...\n",
      "Scraping job 9 of 20 on page 4...\n",
      "Scraping job 10 of 20 on page 4...\n",
      "Scraping job 11 of 20 on page 4...\n",
      "Scraping job 12 of 20 on page 4...\n",
      "Scraping job 13 of 20 on page 4...\n",
      "Scraping job 14 of 20 on page 4...\n",
      "Scraping job 15 of 20 on page 4...\n",
      "Scraping job 16 of 20 on page 4...\n",
      "Scraping job 17 of 20 on page 4...\n",
      "Scraping job 18 of 20 on page 4...\n",
      "Scraping job 19 of 20 on page 4...\n",
      "Scraping job 20 of 20 on page 4...\n",
      "Scraping job 1 of 20 on page 5...\n",
      "Scraping job 2 of 20 on page 5...\n",
      "Scraping job 3 of 20 on page 5...\n",
      "Scraping job 4 of 20 on page 5...\n",
      "Scraping job 5 of 20 on page 5...\n",
      "Scraping job 6 of 20 on page 5...\n",
      "Scraping job 7 of 20 on page 5...\n",
      "Scraping job 8 of 20 on page 5...\n",
      "Scraping job 9 of 20 on page 5...\n",
      "Scraping job 10 of 20 on page 5...\n",
      "Scraping job 11 of 20 on page 5...\n",
      "Scraping job 12 of 20 on page 5...\n",
      "Scraping job 13 of 20 on page 5...\n",
      "Scraping job 14 of 20 on page 5...\n",
      "Scraping job 15 of 20 on page 5...\n",
      "Scraping job 16 of 20 on page 5...\n",
      "Scraping job 17 of 20 on page 5...\n",
      "Scraping job 18 of 20 on page 5...\n",
      "Scraping job 19 of 20 on page 5...\n",
      "Scraping job 20 of 20 on page 5...\n",
      "Scraping job 1 of 20 on page 6...\n",
      "Scraping job 2 of 20 on page 6...\n",
      "Scraping job 3 of 20 on page 6...\n",
      "Scraping job 4 of 20 on page 6...\n",
      "Scraping job 5 of 20 on page 6...\n",
      "Scraping job 6 of 20 on page 6...\n",
      "Scraping job 7 of 20 on page 6...\n",
      "Scraping job 8 of 20 on page 6...\n",
      "Scraping job 9 of 20 on page 6...\n",
      "Scraping job 10 of 20 on page 6...\n",
      "Scraping job 11 of 20 on page 6...\n",
      "Scraping job 12 of 20 on page 6...\n",
      "Scraping job 13 of 20 on page 6...\n",
      "Scraping job 14 of 20 on page 6...\n",
      "Scraping job 15 of 20 on page 6...\n",
      "Scraping job 16 of 20 on page 6...\n",
      "Scraping job 17 of 20 on page 6...\n",
      "Scraping job 18 of 20 on page 6...\n",
      "Scraping job 19 of 20 on page 6...\n",
      "Scraping job 20 of 20 on page 6...\n",
      "Scraping job 1 of 20 on page 7...\n",
      "Scraping job 2 of 20 on page 7...\n",
      "Scraping job 3 of 20 on page 7...\n",
      "Scraping job 4 of 20 on page 7...\n",
      "Scraping job 5 of 20 on page 7...\n",
      "Scraping job 6 of 20 on page 7...\n",
      "Scraping job 7 of 20 on page 7...\n",
      "Scraping job 8 of 20 on page 7...\n",
      "Scraping job 9 of 20 on page 7...\n",
      "Scraping job 10 of 20 on page 7...\n",
      "Scraping job 11 of 20 on page 7...\n",
      "Scraping job 12 of 20 on page 7...\n",
      "Scraping job 13 of 20 on page 7...\n",
      "Scraping job 14 of 20 on page 7...\n",
      "Scraping job 15 of 20 on page 7...\n",
      "Scraping job 16 of 20 on page 7...\n",
      "Scraping job 17 of 20 on page 7...\n",
      "Scraping job 18 of 20 on page 7...\n",
      "Scraping job 19 of 20 on page 7...\n",
      "Scraping job 20 of 20 on page 7...\n",
      "Scraping job 1 of 20 on page 8...\n",
      "Scraping job 2 of 20 on page 8...\n",
      "Scraping job 3 of 20 on page 8...\n",
      "Scraping job 4 of 20 on page 8...\n",
      "Scraping job 5 of 20 on page 8...\n",
      "Scraping job 6 of 20 on page 8...\n",
      "Scraping job 7 of 20 on page 8...\n",
      "Scraping job 8 of 20 on page 8...\n",
      "Scraping job 9 of 20 on page 8...\n",
      "Scraping job 10 of 20 on page 8...\n",
      "Scraping job 11 of 20 on page 8...\n",
      "Scraping job 12 of 20 on page 8...\n",
      "Scraping job 13 of 20 on page 8...\n",
      "Scraping job 14 of 20 on page 8...\n",
      "Scraping job 15 of 20 on page 8...\n",
      "Scraping job 16 of 20 on page 8...\n",
      "Scraping job 17 of 20 on page 8...\n",
      "Scraping job 18 of 20 on page 8...\n",
      "Scraping job 19 of 20 on page 8...\n",
      "Scraping job 20 of 20 on page 8...\n",
      "Scraping job 1 of 20 on page 9...\n",
      "Scraping job 2 of 20 on page 9...\n",
      "Scraping job 3 of 20 on page 9...\n",
      "Scraping job 4 of 20 on page 9...\n",
      "Scraping job 5 of 20 on page 9...\n",
      "Scraping job 6 of 20 on page 9...\n",
      "Scraping job 7 of 20 on page 9...\n",
      "Scraping job 8 of 20 on page 9...\n",
      "Scraping job 9 of 20 on page 9...\n",
      "Scraping job 10 of 20 on page 9...\n",
      "Scraping job 11 of 20 on page 9...\n",
      "Scraping job 12 of 20 on page 9...\n",
      "Scraping job 13 of 20 on page 9...\n",
      "Scraping job 14 of 20 on page 9...\n",
      "Scraping job 15 of 20 on page 9...\n",
      "Scraping job 16 of 20 on page 9...\n",
      "Scraping job 17 of 20 on page 9...\n",
      "Scraping job 18 of 20 on page 9...\n",
      "Scraping job 19 of 20 on page 9...\n",
      "Scraping job 20 of 20 on page 9...\n",
      "Scraping job 1 of 20 on page 10...\n",
      "Scraping job 2 of 20 on page 10...\n",
      "Scraping job 3 of 20 on page 10...\n",
      "Scraping job 4 of 20 on page 10...\n",
      "Scraping job 5 of 20 on page 10...\n",
      "Scraping job 6 of 20 on page 10...\n",
      "Scraping job 7 of 20 on page 10...\n",
      "Scraping job 8 of 20 on page 10...\n",
      "Scraping job 9 of 20 on page 10...\n",
      "Scraping job 10 of 20 on page 10...\n",
      "Scraping job 11 of 20 on page 10...\n",
      "Scraping job 12 of 20 on page 10...\n",
      "Scraping job 13 of 20 on page 10...\n",
      "Scraping job 14 of 20 on page 10...\n",
      "Scraping job 15 of 20 on page 10...\n",
      "Scraping job 16 of 20 on page 10...\n",
      "Scraping job 17 of 20 on page 10...\n",
      "Scraping job 18 of 20 on page 10...\n",
      "Scraping job 19 of 20 on page 10...\n",
      "Scraping job 20 of 20 on page 10...\n",
      "Scraping job 1 of 20 on page 11...\n",
      "Scraping job 2 of 20 on page 11...\n",
      "Scraping job 3 of 20 on page 11...\n",
      "Scraping job 4 of 20 on page 11...\n",
      "Scraping job 5 of 20 on page 11...\n",
      "Scraping job 6 of 20 on page 11...\n",
      "Scraping job 7 of 20 on page 11...\n",
      "Scraping job 8 of 20 on page 11...\n",
      "Scraping job 9 of 20 on page 11...\n",
      "Scraping job 10 of 20 on page 11...\n",
      "Scraping job 11 of 20 on page 11...\n",
      "Scraping job 12 of 20 on page 11...\n",
      "Scraping job 13 of 20 on page 11...\n",
      "Scraping job 14 of 20 on page 11...\n",
      "Scraping job 15 of 20 on page 11...\n",
      "Scraping job 16 of 20 on page 11...\n",
      "Scraping job 17 of 20 on page 11...\n",
      "Scraping job 18 of 20 on page 11...\n",
      "Scraping job 19 of 20 on page 11...\n",
      "Scraping job 20 of 20 on page 11...\n",
      "Scraping job 1 of 20 on page 12...\n",
      "Scraping job 2 of 20 on page 12...\n",
      "Scraping job 3 of 20 on page 12...\n",
      "Scraping job 4 of 20 on page 12...\n",
      "Scraping job 5 of 20 on page 12...\n",
      "Scraping job 6 of 20 on page 12...\n",
      "Scraping job 7 of 20 on page 12...\n",
      "Scraping job 8 of 20 on page 12...\n",
      "Scraping job 9 of 20 on page 12...\n",
      "Scraping job 10 of 20 on page 12...\n",
      "Scraping job 11 of 20 on page 12...\n",
      "Scraping job 12 of 20 on page 12...\n",
      "Scraping job 13 of 20 on page 12...\n",
      "Scraping job 14 of 20 on page 12...\n",
      "Scraping job 15 of 20 on page 12...\n",
      "Scraping job 16 of 20 on page 12...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 17 of 20 on page 12...\n",
      "Scraping job 18 of 20 on page 12...\n",
      "Scraping job 19 of 20 on page 12...\n",
      "Scraping job 20 of 20 on page 12...\n",
      "Scraping job 1 of 20 on page 13...\n",
      "Scraping job 2 of 20 on page 13...\n",
      "Scraping job 3 of 20 on page 13...\n",
      "Scraping job 4 of 20 on page 13...\n",
      "Scraping job 5 of 20 on page 13...\n",
      "Scraping job 6 of 20 on page 13...\n",
      "Scraping job 7 of 20 on page 13...\n",
      "Scraping job 8 of 20 on page 13...\n",
      "Scraping job 9 of 20 on page 13...\n",
      "Scraping job 10 of 20 on page 13...\n",
      "Scraping job 11 of 20 on page 13...\n",
      "Scraping job 12 of 20 on page 13...\n",
      "Scraping job 13 of 20 on page 13...\n",
      "Scraping job 14 of 20 on page 13...\n",
      "Scraping job 15 of 20 on page 13...\n",
      "Scraping job 16 of 20 on page 13...\n",
      "Scraping job 17 of 20 on page 13...\n",
      "Scraping job 18 of 20 on page 13...\n",
      "Scraping job 19 of 20 on page 13...\n",
      "Scraping job 20 of 20 on page 13...\n",
      "Scraping job 1 of 20 on page 14...\n",
      "Scraping job 2 of 20 on page 14...\n",
      "Scraping job 3 of 20 on page 14...\n",
      "Scraping job 4 of 20 on page 14...\n",
      "Scraping job 5 of 20 on page 14...\n",
      "Scraping job 6 of 20 on page 14...\n",
      "Scraping job 7 of 20 on page 14...\n",
      "Scraping job 8 of 20 on page 14...\n",
      "Scraping job 9 of 20 on page 14...\n",
      "Scraping job 10 of 20 on page 14...\n",
      "Scraping job 11 of 20 on page 14...\n",
      "Scraping job 12 of 20 on page 14...\n",
      "Scraping job 13 of 20 on page 14...\n",
      "Scraping job 14 of 20 on page 14...\n",
      "Scraping job 15 of 20 on page 14...\n",
      "Scraping job 16 of 20 on page 14...\n",
      "Scraping job 17 of 20 on page 14...\n",
      "Scraping job 18 of 20 on page 14...\n",
      "Scraping job 19 of 20 on page 14...\n",
      "Scraping job 20 of 20 on page 14...\n",
      "Scraping job 1 of 20 on page 15...\n",
      "Scraping job 2 of 20 on page 15...\n",
      "Scraping job 3 of 20 on page 15...\n",
      "Scraping job 4 of 20 on page 15...\n",
      "Scraping job 5 of 20 on page 15...\n",
      "Scraping job 6 of 20 on page 15...\n",
      "Scraping job 7 of 20 on page 15...\n",
      "Scraping job 8 of 20 on page 15...\n",
      "Scraping job 9 of 20 on page 15...\n",
      "Scraping job 10 of 20 on page 15...\n",
      "Scraping job 11 of 20 on page 15...\n",
      "Scraping job 12 of 20 on page 15...\n",
      "Scraping job 13 of 20 on page 15...\n",
      "Scraping job 14 of 20 on page 15...\n",
      "Scraping job 15 of 20 on page 15...\n",
      "Scraping job 16 of 20 on page 15...\n",
      "Scraping job 17 of 20 on page 15...\n",
      "Scraping job 18 of 20 on page 15...\n",
      "Scraping job 19 of 20 on page 15...\n",
      "Scraping job 20 of 20 on page 15...\n",
      "Scraping job 1 of 20 on page 16...\n",
      "Scraping job 2 of 20 on page 16...\n",
      "Scraping job 3 of 20 on page 16...\n",
      "Scraping job 4 of 20 on page 16...\n",
      "Scraping job 5 of 20 on page 16...\n",
      "Scraping job 6 of 20 on page 16...\n",
      "Scraping job 7 of 20 on page 16...\n",
      "Scraping job 8 of 20 on page 16...\n",
      "Scraping job 9 of 20 on page 16...\n",
      "Scraping job 10 of 20 on page 16...\n",
      "Scraping job 11 of 20 on page 16...\n",
      "Scraping job 12 of 20 on page 16...\n",
      "Scraping job 13 of 20 on page 16...\n",
      "Scraping job 14 of 20 on page 16...\n",
      "Scraping job 15 of 20 on page 16...\n",
      "Scraping job 16 of 20 on page 16...\n",
      "Scraping job 17 of 20 on page 16...\n",
      "Scraping job 18 of 20 on page 16...\n",
      "Scraping job 19 of 20 on page 16...\n",
      "Scraping job 20 of 20 on page 16...\n",
      "Scraping job 1 of 20 on page 17...\n",
      "Scraping job 2 of 20 on page 17...\n",
      "Scraping job 3 of 20 on page 17...\n",
      "Scraping job 4 of 20 on page 17...\n",
      "Scraping job 5 of 20 on page 17...\n",
      "Scraping job 6 of 20 on page 17...\n",
      "Scraping job 7 of 20 on page 17...\n",
      "Scraping job 8 of 20 on page 17...\n",
      "Scraping job 9 of 20 on page 17...\n",
      "Scraping job 10 of 20 on page 17...\n",
      "Scraping job 11 of 20 on page 17...\n",
      "Scraping job 12 of 20 on page 17...\n",
      "Scraping job 13 of 20 on page 17...\n",
      "Scraping job 14 of 20 on page 17...\n",
      "Scraping job 15 of 20 on page 17...\n",
      "Scraping job 16 of 20 on page 17...\n",
      "Scraping job 17 of 20 on page 17...\n",
      "Scraping job 18 of 20 on page 17...\n",
      "Scraping job 19 of 20 on page 17...\n",
      "Scraping job 20 of 20 on page 17...\n",
      "Scraping job 1 of 9 on page 18...\n",
      "Scraping job 2 of 9 on page 18...\n",
      "Scraping job 3 of 9 on page 18...\n",
      "Scraping job 4 of 9 on page 18...\n",
      "Scraping job 5 of 9 on page 18...\n",
      "Scraping job 6 of 9 on page 18...\n",
      "Scraping job 7 of 9 on page 18...\n",
      "Scraping job 8 of 9 on page 18...\n",
      "Scraping job 9 of 9 on page 18...\n",
      "Done scraping all data.\n"
     ]
    }
   ],
   "source": [
    "wow_seek_scraper = SeekJobScraper(\n",
    "    total_pages=18,\n",
    "    raw_url='https://www.seek.com.au/api/chalice-search/v4/search?siteKey=AU-Main&sourcesystem=houston&userqueryid=374a27e499beb4e0f37790206a2bc966-5035864&userid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&usersessionid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&eventCaptureSessionId=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&where=All+Australia&seekSelectAllPages=true&companyname=Woolworths+Group&include=seodata&locale=en-AU&solId=8e8f007e-f45c-4c00-913d-7c862f50c2cb'\n",
    ")\n",
    "\n",
    "wow_seek_scraper.scrape(sleep_duration=2)\n",
    "\n",
    "wow_seek_jobs = wow_seek_scraper.save_data('data/jobs_seek/wow_jobs_seek.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23cb637e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [company, job_title, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_null_rows(wow_seek_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe314fde",
   "metadata": {},
   "source": [
    "# Scraping Telstra jobs on Seek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7402144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping job 1 of 20 on page 1...\n",
      "Scraping job 2 of 20 on page 1...\n",
      "Scraping job 3 of 20 on page 1...\n",
      "Scraping job 4 of 20 on page 1...\n",
      "Scraping job 5 of 20 on page 1...\n",
      "Scraping job 6 of 20 on page 1...\n",
      "Scraping job 7 of 20 on page 1...\n",
      "Scraping job 8 of 20 on page 1...\n",
      "Scraping job 9 of 20 on page 1...\n",
      "Scraping job 10 of 20 on page 1...\n",
      "Scraping job 11 of 20 on page 1...\n",
      "Scraping job 12 of 20 on page 1...\n",
      "Scraping job 13 of 20 on page 1...\n",
      "Scraping job 14 of 20 on page 1...\n",
      "Scraping job 15 of 20 on page 1...\n",
      "Scraping job 16 of 20 on page 1...\n",
      "Scraping job 17 of 20 on page 1...\n",
      "Scraping job 18 of 20 on page 1...\n",
      "Scraping job 19 of 20 on page 1...\n",
      "Scraping job 20 of 20 on page 1...\n",
      "Scraping job 1 of 20 on page 2...\n",
      "Scraping job 2 of 20 on page 2...\n",
      "Scraping job 3 of 20 on page 2...\n",
      "Scraping job 4 of 20 on page 2...\n",
      "Scraping job 5 of 20 on page 2...\n",
      "Scraping job 6 of 20 on page 2...\n",
      "Scraping job 7 of 20 on page 2...\n",
      "Scraping job 8 of 20 on page 2...\n",
      "Scraping job 9 of 20 on page 2...\n",
      "Scraping job 10 of 20 on page 2...\n",
      "Scraping job 11 of 20 on page 2...\n",
      "Scraping job 12 of 20 on page 2...\n",
      "Scraping job 13 of 20 on page 2...\n",
      "Scraping job 14 of 20 on page 2...\n",
      "Scraping job 15 of 20 on page 2...\n",
      "Scraping job 16 of 20 on page 2...\n",
      "Scraping job 17 of 20 on page 2...\n",
      "Scraping job 18 of 20 on page 2...\n",
      "Scraping job 19 of 20 on page 2...\n",
      "Scraping job 20 of 20 on page 2...\n",
      "Scraping job 1 of 20 on page 3...\n",
      "Scraping job 2 of 20 on page 3...\n",
      "Scraping job 3 of 20 on page 3...\n",
      "Scraping job 4 of 20 on page 3...\n",
      "Scraping job 5 of 20 on page 3...\n",
      "Scraping job 6 of 20 on page 3...\n",
      "Scraping job 7 of 20 on page 3...\n",
      "Scraping job 8 of 20 on page 3...\n",
      "Scraping job 9 of 20 on page 3...\n",
      "Scraping job 10 of 20 on page 3...\n",
      "Scraping job 11 of 20 on page 3...\n",
      "Scraping job 12 of 20 on page 3...\n",
      "Scraping job 13 of 20 on page 3...\n",
      "Scraping job 14 of 20 on page 3...\n",
      "Scraping job 15 of 20 on page 3...\n",
      "Scraping job 16 of 20 on page 3...\n",
      "Scraping job 17 of 20 on page 3...\n",
      "Scraping job 18 of 20 on page 3...\n",
      "Scraping job 19 of 20 on page 3...\n",
      "Scraping job 20 of 20 on page 3...\n",
      "Scraping job 1 of 20 on page 4...\n",
      "Scraping job 2 of 20 on page 4...\n",
      "Scraping job 3 of 20 on page 4...\n",
      "Scraping job 4 of 20 on page 4...\n",
      "Scraping job 5 of 20 on page 4...\n",
      "Scraping job 6 of 20 on page 4...\n",
      "Scraping job 7 of 20 on page 4...\n",
      "Scraping job 8 of 20 on page 4...\n",
      "Scraping job 9 of 20 on page 4...\n",
      "Scraping job 10 of 20 on page 4...\n",
      "Scraping job 11 of 20 on page 4...\n",
      "Scraping job 12 of 20 on page 4...\n",
      "Scraping job 13 of 20 on page 4...\n",
      "Scraping job 14 of 20 on page 4...\n",
      "Scraping job 15 of 20 on page 4...\n",
      "Scraping job 16 of 20 on page 4...\n",
      "Scraping job 17 of 20 on page 4...\n",
      "Scraping job 18 of 20 on page 4...\n",
      "Scraping job 19 of 20 on page 4...\n",
      "Scraping job 20 of 20 on page 4...\n",
      "Scraping job 1 of 20 on page 5...\n",
      "Scraping job 2 of 20 on page 5...\n",
      "Scraping job 3 of 20 on page 5...\n",
      "Scraping job 4 of 20 on page 5...\n",
      "Scraping job 5 of 20 on page 5...\n",
      "Scraping job 6 of 20 on page 5...\n",
      "Scraping job 7 of 20 on page 5...\n",
      "Scraping job 8 of 20 on page 5...\n",
      "Scraping job 9 of 20 on page 5...\n",
      "Scraping job 10 of 20 on page 5...\n",
      "Scraping job 11 of 20 on page 5...\n",
      "Scraping job 12 of 20 on page 5...\n",
      "Scraping job 13 of 20 on page 5...\n",
      "Scraping job 14 of 20 on page 5...\n",
      "Scraping job 15 of 20 on page 5...\n",
      "Scraping job 16 of 20 on page 5...\n",
      "Scraping job 17 of 20 on page 5...\n",
      "Scraping job 18 of 20 on page 5...\n",
      "Scraping job 19 of 20 on page 5...\n",
      "Scraping job 20 of 20 on page 5...\n",
      "Scraping job 1 of 20 on page 6...\n",
      "Scraping job 2 of 20 on page 6...\n",
      "Scraping job 3 of 20 on page 6...\n",
      "Scraping job 4 of 20 on page 6...\n",
      "Scraping job 5 of 20 on page 6...\n",
      "Scraping job 6 of 20 on page 6...\n",
      "Scraping job 7 of 20 on page 6...\n",
      "Scraping job 8 of 20 on page 6...\n",
      "Scraping job 9 of 20 on page 6...\n",
      "Scraping job 10 of 20 on page 6...\n",
      "Scraping job 11 of 20 on page 6...\n",
      "Scraping job 12 of 20 on page 6...\n",
      "Scraping job 13 of 20 on page 6...\n",
      "Scraping job 14 of 20 on page 6...\n",
      "Scraping job 15 of 20 on page 6...\n",
      "Scraping job 16 of 20 on page 6...\n",
      "Scraping job 17 of 20 on page 6...\n",
      "Scraping job 18 of 20 on page 6...\n",
      "Scraping job 19 of 20 on page 6...\n",
      "Scraping job 20 of 20 on page 6...\n",
      "Scraping job 1 of 5 on page 7...\n",
      "Scraping job 2 of 5 on page 7...\n",
      "Scraping job 3 of 5 on page 7...\n",
      "Scraping job 4 of 5 on page 7...\n",
      "Scraping job 5 of 5 on page 7...\n",
      "Done scraping all data.\n"
     ]
    }
   ],
   "source": [
    "tls_seek_scraper = SeekJobScraper(\n",
    "    total_pages=7,\n",
    "    raw_url='https://www.seek.com.au/api/chalice-search/v4/search?siteKey=AU-Main&sourcesystem=houston&userqueryid=8a9f4c84f660d4709c8a71bcfa927813-5698512&userid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&usersessionid=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&eventCaptureSessionId=fb3867e8-2415-49bb-963a-a9cbdd9d7a23&where=All+Australia&seekSelectAllPages=true&companyname=Telstra&include=seodata&locale=en-AU&solId=8e8f007e-f45c-4c00-913d-7c862f50c2cb'\n",
    ")\n",
    "\n",
    "tls_seek_scraper.scrape(sleep_duration=2)\n",
    "\n",
    "tls_seek_jobs = tls_seek_scraper.save_data('data/jobs_seek/tls_jobs_seek.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df093a49",
   "metadata": {},
   "source": [
    "# Full Seek Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc00c42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Commonwealth Bank of Australia</td>\n",
       "      <td>Business Banking Customer Service Representative</td>\n",
       "      <td>Work from home options availableStructured opp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Commonwealth Bank of Australia</td>\n",
       "      <td>HR Advisor</td>\n",
       "      <td>See yourself in our team:  People are a key pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Commonwealth Bank of Australia</td>\n",
       "      <td>Customer Service Representative - CommSec</td>\n",
       "      <td>You are available to work Monday to Friday bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Commonwealth Bank of Australia</td>\n",
       "      <td>Customer Service Representative - CommSec</td>\n",
       "      <td>You are available to work Monday to Friday bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Commonwealth Bank of Australia</td>\n",
       "      <td>Customer Service Specialist</td>\n",
       "      <td>About us:   The Customer Service Remediation (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          company  \\\n",
       "0  Commonwealth Bank of Australia   \n",
       "1  Commonwealth Bank of Australia   \n",
       "2  Commonwealth Bank of Australia   \n",
       "3  Commonwealth Bank of Australia   \n",
       "4  Commonwealth Bank of Australia   \n",
       "\n",
       "                                          job_title  \\\n",
       "0  Business Banking Customer Service Representative   \n",
       "1                                        HR Advisor   \n",
       "2         Customer Service Representative - CommSec   \n",
       "3         Customer Service Representative - CommSec   \n",
       "4                       Customer Service Specialist   \n",
       "\n",
       "                                         description  \n",
       "0  Work from home options availableStructured opp...  \n",
       "1  See yourself in our team:  People are a key pa...  \n",
       "2  You are available to work Monday to Friday bet...  \n",
       "3  You are available to work Monday to Friday bet...  \n",
       "4  About us:   The Customer Service Remediation (...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cba_seek_jobs = pd.read_csv('data/jobs_seek/cba_jobs_seek.csv')\n",
    "csl_seek_jobs = pd.read_csv('data/jobs_seek/csl_jobs_seek.csv')\n",
    "bhp_seek_jobs = pd.read_csv('data/jobs_seek/bhp_jobs_seek.csv')\n",
    "wow_seek_jobs = pd.read_csv('data/jobs_seek/wow_jobs_seek.csv')\n",
    "tls_seek_jobs = pd.read_csv('data/jobs_seek/tls_jobs_seek.csv')\n",
    "\n",
    "full_seek_data = [\n",
    "    cba_seek_jobs,\n",
    "    csl_seek_jobs,\n",
    "    bhp_seek_jobs,\n",
    "    wow_seek_jobs,\n",
    "    tls_seek_jobs\n",
    "]\n",
    "\n",
    "full_seek_data_df = pd.concat(full_seek_data, ignore_index=True)\n",
    "full_seek_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c11c81a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_seek_data_df.to_csv('data/jobs_seek/all_jobs_seek.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "099657e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_seek_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a472abaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [company, job_title, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_null_rows(full_seek_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829885e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
